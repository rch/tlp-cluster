= tlp-cluster
Jon Haddad <jon@thelastpickle.com>
:toc: left
:icons: font

This is the manual for tlp-cluster, a provisioning tool for Apache Cassandra designed for developers looking to benchmark and test Apache Cassandra.  It assists with builds and starting instances on AWS.

If you are looking for a tool to aid in benchmarking these clusters please see the companion project http://thelastpickle.com/tlp-stress/[tlp-stress].

If you're looking for tools to help manage Cassandra in _production_ environments please see the http://cassandra-reaper.io/[Reaper] project and https://github.com/spotify/cstar[cstar]

== Prerequisites

- An AWS access key and secret.  tlp-cluster uses https://www.terraform.io/[Terraform] to create and destroy instances.  You will be prompted for these the first time you start tlp-cluster.
- The access key needs permissions to create an S3 bucket as well as create SSH keys.  Separate keys are used by default for security reasons.

== Installation

The easiest way to get started is to use one of our prebuilt packages.

=== Installing a Package

The easiest way to get started is to use your favorite package manager.

The current version is {TLP_CLUSTER_VERSION}.

==== Deb Packages

```
$ echo "deb https://dl.bintray.com/thelastpickle/tlp-tools-deb weezy main" | sudo tee -a /etc/apt/sources.list
$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 2895100917357435
$ sudo apt update
$ sudo apt install tlp-cluster
```

==== RPM Packages

You'll need the bintray repo set up on your machine.  Create this `/etc/yum.repos.d/tlp-tools.repo`:

```
[bintraybintray-thelastpickle-tlp-tools-rpm]
name=bintray-thelastpickle-tlp-tools-rpm
baseurl=https://dl.bintray.com/thelastpickle/tlp-tools-rpm
gpgcheck=0
repo_gpgcheck=0
enabled=1
```

Then run the following to install:

```
$ yum install tlp-cluster
```


==== Tarball Install

If you're using mac, for now you'll need to grab our tarball using:

[subs="attributes"]
```
$ curl -L -O "https://dl.bintray.com/thelastpickle/tlp-tools-tarball/tlp-cluster-{TLP_CLUSTER_VERSION}.tar"
$ tar -xzf tlp-cluster-{TLP_CLUSTER_VERSION}.tar
```

To get started, add the bin directory of tlp-cluster to your $PATH.  For example:

[source,bash]
----
export PATH="$PATH:/path/to/tlp-cluster/bin"
cd /path/to/tlp-cluster
./gradlew assemble
----

== Setup

We currently have a dependency on shell scripts meaning you'll need a Mac or Linux box to use this tool.

If you've never used to the tool before, the first time you run a command you'll be asked to supply some information, which will generate a configuration file which will be placed in your `$HOME/.tlp-cluster/profiles/default/settings.yaml`.

IMPORTANT: We currently only support the ubuntu 16 ami in us-west-2.  We'd love a pull request to improve this!

Running the command without any arguments will print out the usage:

[source,bash]
----
tlp-cluster
----

You'll see the help.  It looks like this:

```
include::includes/help.txt[]
```

== Initialize a Cluster

The tool uses the current working directory as a project, of sorts. To get started, run the following, substituting your customer / project, ticket and purpose.

[source,bash]
----
tlp-cluster init CLIENT TICKET PURPOSE
----

Where:

* `CLIENT` - Name of the customer, client, or project associated with the work you are doing with `tlp-cluster`.
* `TICKET` - Jira or github ticket number associated with the work you are doing with `tlp-cluster`.
* `PURPOSE` - Reason why you are creating the cluster.

This will initialize the current directory with a terraform.tf.json. You can open this up in an editor. Here you can change the number of nodes in the cluster, as well as configure the number of stress nodes you want. You can also change the instance type.  Generally speaking though, you shouldn't have to do this.  If you find yourself doing it often, please submit an https://github.com/thelastpickle/tlp-cluster/issues[issue] describing your requirements and we'll work with you to solve the problem.

Certain instances types may not work with the ami that's hard coded at the moment, we're looking to fix / improve this.


== Launch Instances

Launch your instances with the following:

[source,bash]
----
tlp-cluster up
----

Terraform will eventually ask you to type `yes` and fire up your instances.  Optionally you can pass `--yes` to the `-up` command and you won't be prompted.

tlp-cluster will create a file, `env.sh`, which has helpful aliases and bash functions that will help you run your cluster.  Run the following:

[source,bash]
----
source env.sh
----

This will set up SSH, SCP, SFTP, and rsync to use a local sshConfig file, as well as some other helpful aliases.


SSH alises for all Cassandra nodes are automatically created as `c0`-`cN`.  `ssh` is not required.  For example:

`c0 nodetool status`

In addition, the following are defined:

* *c_all* executes a command on every node in the cluster sequentially.
* *x_all* executes a command on every node in the cluster concurrently.
* *c_collect_artifacts* will collect metrics, nodetool output and system information into the artifacts directory.  It takes a name as a parameter.  This is useful when doing performance testing to capture the state of the system at a given moment.
* *c_start* Starts cassandra on all nodes.
* *c_restart* Restarts cassandra on all nodes.  Not a graceful operation.  To test true rolling restarts we recommend using https://github.com/spotify/cstar[cstar].
* *c_status*: Executes `nodetool status` on cassandra0.
* *c_tpstats*: Executes `nodetool tpstats` on all nodes.


== Installing Cassandra

=== The Easy Way - Use a Released Build

The easiest path forward to getting a cluster up and running is the following:

[source,bash]
----
tlp-cluster use 3.11.4
tlp-cluster install
tlp-cluster start
----

Simply replace 3.11.4 with the release version.




=== The Hard Way - Use a custom Build

To install Cassandra on your instances, you will need to follow these steps:

1. Build the version you need and give it a build name (optional)
2. Tell tlp-cluster to use the custom build

The first step is optional because you may already have a build in the `~/.tlp-cluster/build` directory that you want to use.

If you have no builds you will need to run the following:

[source,bash]
----
tlp-cluster build -n BUILD_NAME /path/to/repo
----

Where:

* `BUILD_NAME` - Name you want to give the build e.g. my-build-cass-4.0.
* `/path/to/repo` - Full path to clone of the Cassandra repository.

If you already have a build that you would like to use you can run the following:

[source,bash]
----
tlp-cluster use BUILD_NAME
----

This will copy the binaries and configuration files to the `provisioning/cassandra` directory in your `tlp-cluster` repository. The `provisioning` directory contains a number of files that can be used to set up your instances. Being realistic, since we do so much non-standard work (EBS vs instance store, LVM vs FS directly on a device, caches, etc) we need the ability to run arbitrary commands. This isnâ€™t a great use case for puppet / chef / salt / ansible (yet), so we are just using easy to modify scripts for now.

If you want to install other binaries or perform other operations during provisioning of the instances, you can add them to the `provisioning/cassandra` directory. Note that any new scripts you add should be prefixed with a number which is used to determine the order they are executed by the `install.sh` script.

To provision the instances run the following:

[source,bash]
----
tlp-cluster install
----

Where:

* `SSH_KEY_PATH` - Is the full path to the private key from the key pair used when creating the instances.

This will push the contents of the `provisioning/cassandra` directory up to each of the instances you have created and install Cassandra on them.

The following ports are open:

[options="header"]
|===
|Port|Purpose
|3000|Grafana web interface
|9090|Prometheus Web Interface
|9042|Cassandra Native Protocol (cql)
|9500|tlp-stress prometheus port
|9103|Cassandra prometheus port
|===


the normal ports are all mapped for you so you can reach prometheus on 9090 and grafana on 3000

== Add ons

=== Express cluster build

A helper script named `build_cluster.sh` has been added to the `bin/` directory to automate cluster creation and startup:

```
% build_cluster.sh -h
build-cluster: automation script for tlp-cluster


options:
-h, --help                                  show brief help
-n, --name=CLUSTER_NAME                     Cluster name
-s, --stress=STRESS_NODES                   specify the number of stress nodes
-v, --cassandra-version=CASSANDRA_VERSION   specify the version of Cassandra to install
-d, --extra-deb-package=EXTRA_DEB           optional deb package to install on the nodes
-c, --cassandra-nodes=3                     number of Cassandra nodes to start (default: 3)
-i, --instance-type=r3.2xlarge              Instance type for Cassandra nodes (default: m3.xlarge)
-d, --extra-deb-package=EXTRA_DEB           optional deb package to install on the nodes
--gc=G1                                     GC algorithm to use. Possible values: G1, Shenandoah, ZGC, CMS (default)
--heap=8                                    Heap size in GB (8, 16, 32, ...)
--jdk=8                                     OpenJDK version to use (8, 11, 14)
```

The script will pause before the install phase to allow customizing the configuration before it gets deployed. It'll also retry on failure during the install and start phases to handle transient failures.

=== Install and configure Reaper

Reaper can be installed in sidecar mode after the cluster was created using `bin/install_reaper.sh`:

```
% install_reaper.sh -h
install_reaper: automation script for tlp-cluster


options:
-h, --help                                  show brief help
-b                                          Install Reaper latest beta
--cluster=CLUSTER_NAME                      Cluster on which Reaper should be installed
```

The cluster name is not required if the script runs from within the cluster directory created by tlp-cluster.
The latest stable version will be installed by default, unless the `-b` flag is used to enforce using the latest beta instead.
Connect to any node on port 8080 after the installation has succeeded and use admin/admin as default credentials.

=== Install and configure Medusa

Install and configure Medusa using S3 as storage backend.

```
% install_medusa.sh -h
install_medusa: automation script for tlp-cluster


options:
-h, --help                                  show brief help
-b, --bucket=bucket-name                    S3 storage bucket name
-c, --credentials=/path/to/credentials      AWS credentials file (defaults to ~/.aws/credentials)
-s, --storage-provider=CASSANDRA_VERSION    Libcloud storage provider (defaults to s3_us_west_oregon)
--cluster=CLUSTER_NAME                      Cluster on which Medusa should be installed
```

The cluster name is not required if the script runs from within the cluster directory created by tlp-cluster.
AWS credentials will get uploaded to the Cassandra nodes under `/etc/medusa/credentials`.
